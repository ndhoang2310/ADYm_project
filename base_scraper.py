import logging
from abc import ABC, abstractmethod
from datetime import datetime
from pymongo import MongoClient
from pymongo.errors import DuplicateKeyError

# C·∫•u h√¨nh Logging (ƒë·ªÉ in ra m√†n h√¨nh tr·∫°ng th√°i ch·∫°y)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(name)s] - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

class BaseScraper(ABC):
    """
    Class cha cho t·∫•t c·∫£ c√°c Scraper.
    Nhi·ªám v·ª•: Qu·∫£n l√Ω k·∫øt n·ªëi Database v√† h√†m l∆∞u d·ªØ li·ªáu chung.
    """

    def __init__(self, source_name, db_name="VietnamITMarket", collection_name="raw_jobs"):
        # 1. ƒê·ªãnh danh: Scraper n√†y t√™n g√¨? (itviec, topcv, hay linkedin?)
        self.source = source_name
        
        # 2. T·∫°o Logger ri√™ng ƒë·ªÉ d·ªÖ theo d√µi l·ªói
        self.logger = logging.getLogger(self.source)

        # 3. K·∫øt n·ªëi MongoDB
        try:
            self.client = MongoClient("mongodb://localhost:27017/")
            self.db = self.client[db_name]
            self.collection = self.db[collection_name]
            self.logger.info(f"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB: {db_name}.{collection_name}")
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}")
            raise e

    def save_job(self, job_data):
        """
        H√†m quan tr·ªçng nh·∫•t: L∆∞u job v√†o DB v√† x·ª≠ l√Ω tr√πng l·∫∑p.
        """
        # A. T·ª± ƒë·ªông ƒëi·ªÅn c√°c th√¥ng tin qu·∫£n l√Ω (Metadata)
        job_data['source'] = self.source
        job_data['crawled_date'] = datetime.now()  # Th·ªùi ƒëi·ªÉm hi·ªán t·∫°i

        # B. Ki·ªÉm tra s∆° b·ªô (Validation) - ƒë·∫£m b·∫£o c√≥ URL v√† Title
        if 'url' not in job_data or not job_data['url']:
            self.logger.warning("‚ö†Ô∏è B·ªè qua Job thi·∫øu URL")
            return

        # C. Th·ª≠ l∆∞u v√†o Database
        try:
            # insert_one: L·ªánh c·ªßa Mongo ƒë·ªÉ th√™m 1 b·∫£n ghi
            self.collection.insert_one(job_data)
            self.logger.info(f"üíæ ƒê√£ l∆∞u: {job_data.get('job_title', 'Unknown')} ({job_data['url']})")
        
        except DuplicateKeyError:
            # N·∫øu tr√πng URL (do index unique), Mongo s·∫Ω b√°o l·ªói n√†y.
            # Ta b·∫Øt l·ªói l·∫°i v√† ch·ªâ in ra c·∫£nh b√°o, kh√¥ng l√†m s·∫≠p ch∆∞∆°ng tr√¨nh.
            self.logger.warning(f"‚è© ƒê√£ t·ªìn t·∫°i (B·ªè qua): {job_data['url']}")
        
        except Exception as e:
            # C√°c l·ªói kh√°c (sai ƒë·ªãnh d·∫°ng, m·∫•t m·∫°ng...)
            self.logger.error(f"‚ùå L·ªói khi l∆∞u: {e}")

    @abstractmethod
    def scrape(self):
        """
        H√†m tr·ª´u t∆∞·ª£ng.
        Class cha kh√¥ng vi·∫øt g√¨ ·ªü ƒë√¢y c·∫£.
        B·∫Øt bu·ªôc Class con (Dev A, Dev B) ph·∫£i t·ª± vi·∫øt logic c√†o c·ªßa ri√™ng h·ªç.
        """
        pass

    def close_connection(self):
        """ƒê√≥ng k·∫øt n·ªëi khi ch·∫°y xong ƒë·ªÉ gi·∫£i ph√≥ng t√†i nguy√™n"""
        self.client.close()
        self.logger.info("ƒê√£ ƒë√≥ng k·∫øt n·ªëi Database.")import logging
from abc import ABC, abstractmethod
from datetime import datetime
from pymongo import MongoClient
from pymongo.errors import DuplicateKeyError

# C·∫•u h√¨nh Logging (ƒë·ªÉ in ra m√†n h√¨nh tr·∫°ng th√°i ch·∫°y)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(name)s] - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

class BaseScraper(ABC):
    """
    Class cha cho t·∫•t c·∫£ c√°c Scraper.
    Nhi·ªám v·ª•: Qu·∫£n l√Ω k·∫øt n·ªëi Database v√† h√†m l∆∞u d·ªØ li·ªáu chung.
    """

    def __init__(self, source_name, db_name="VietnamITMarket", collection_name="raw_jobs"):
        # 1. ƒê·ªãnh danh: Scraper n√†y t√™n g√¨? (itviec, topcv, hay linkedin?)
        self.source = source_name
        
        # 2. T·∫°o Logger ri√™ng ƒë·ªÉ d·ªÖ theo d√µi l·ªói
        self.logger = logging.getLogger(self.source)

        # 3. K·∫øt n·ªëi MongoDB
        try:
            self.client = MongoClient("mongodb://localhost:27017/")
            self.db = self.client[db_name]
            self.collection = self.db[collection_name]
            self.logger.info(f"‚úÖ ƒê√£ k·∫øt n·ªëi MongoDB: {db_name}.{collection_name}")
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói k·∫øt n·ªëi MongoDB: {e}")
            raise e

    def save_job(self, job_data):
        """
        H√†m quan tr·ªçng nh·∫•t: L∆∞u job v√†o DB v√† x·ª≠ l√Ω tr√πng l·∫∑p.
        """
        # A. T·ª± ƒë·ªông ƒëi·ªÅn c√°c th√¥ng tin qu·∫£n l√Ω (Metadata)
        job_data['source'] = self.source
        job_data['crawled_date'] = datetime.now()  # Th·ªùi ƒëi·ªÉm hi·ªán t·∫°i

        # B. Ki·ªÉm tra s∆° b·ªô (Validation) - ƒë·∫£m b·∫£o c√≥ URL v√† Title
        if 'url' not in job_data or not job_data['url']:
            self.logger.warning("‚ö†Ô∏è B·ªè qua Job thi·∫øu URL")
            return

        # C. Th·ª≠ l∆∞u v√†o Database
        try:
            # insert_one: L·ªánh c·ªßa Mongo ƒë·ªÉ th√™m 1 b·∫£n ghi
            self.collection.insert_one(job_data)
            self.logger.info(f"üíæ ƒê√£ l∆∞u: {job_data.get('job_title', 'Unknown')} ({job_data['url']})")
        
        except DuplicateKeyError:
            # N·∫øu tr√πng URL (do index unique), Mongo s·∫Ω b√°o l·ªói n√†y.
            # Ta b·∫Øt l·ªói l·∫°i v√† ch·ªâ in ra c·∫£nh b√°o, kh√¥ng l√†m s·∫≠p ch∆∞∆°ng tr√¨nh.
            self.logger.warning(f"‚è© ƒê√£ t·ªìn t·∫°i (B·ªè qua): {job_data['url']}")
        
        except Exception as e:
            # C√°c l·ªói kh√°c (sai ƒë·ªãnh d·∫°ng, m·∫•t m·∫°ng...)
            self.logger.error(f"‚ùå L·ªói khi l∆∞u: {e}")

    @abstractmethod
    def scrape(self):
        """
        H√†m tr·ª´u t∆∞·ª£ng.
        Class cha kh√¥ng vi·∫øt g√¨ ·ªü ƒë√¢y c·∫£.
        B·∫Øt bu·ªôc Class con (Dev A, Dev B) ph·∫£i t·ª± vi·∫øt logic c√†o c·ªßa ri√™ng h·ªç.
        """
        pass

    def close_connection(self):
        """ƒê√≥ng k·∫øt n·ªëi khi ch·∫°y xong ƒë·ªÉ gi·∫£i ph√≥ng t√†i nguy√™n"""
        self.client.close()
        self.logger.info("ƒê√£ ƒë√≥ng k·∫øt n·ªëi Database.")