import logging
import time
import random
from datetime import datetime
from base_scraper_new import BaseScraper
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class VietnamWorksScraper(BaseScraper):
    def __init__(self):
        super().__init__(source_name="vietnamworks")
        
    def setup_driver(self):
        options = Options()
        # B·ªè comment l·ªánh d∆∞·ªõi n·∫øu mu·ªën ch·∫°y ·∫©n
        options.add_argument("--headless") 
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        return webdriver.Chrome(options=options)

    def get_job_links_on_page(self, driver, page_url):
        #L·∫•y t·∫•t c·∫£ link vi·ªác l√†m tr√™n 1 trang danh s√°ch
        driver.get(page_url)
        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "h2 a")))
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 2);")
            time.sleep(1)
            
            elements = driver.find_elements(By.CSS_SELECTOR, "h2 a")
            links = []
            for elem in elements:
                href = elem.get_attribute("href")
                if href:
                    if not href.startswith("http"):
                        href = "https://www.vietnamworks.com" + href
                    links.append(href)
            return list(set(links))
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è L·ªói l·∫•y link ·ªü trang danh s√°ch: {e}")
            return []

    def get_text_by_label(self, driver, label_text):
        #H√†m h·ªó tr·ª£: T√¨m gi√° tr·ªã (th·∫ª p) d·ª±a tr√™n nh√£n (th·∫ª label)
        try:
            # XPath t√¨m label ch·ª©a text -> l·∫•y th·∫ª p ngay sau n√≥
            xpath = f"//label[contains(text(), '{label_text}')]/following-sibling::p"
            element = driver.find_element(By.XPATH, xpath)
            return element.text.strip()
        except:
            return None

    def parse_job_detail(self, driver, url):
        """V√†o trang chi ti·∫øt ƒë·ªÉ l·∫•y to√†n b·ªô th√¥ng tin theo Schema chung"""
        driver.get(url)
        time.sleep(random.uniform(1.5, 3)) 
        
        # --- T√åM V√Ä CLICK N√öT "XEM TH√äM" ---
        try:
            # T√¨m n√∫t c√≥ ch·ªØ "Xem th√™m" v√† click ƒë·ªÉ hi·ªán full th√¥ng tin
            see_more_btn = driver.find_element(By.XPATH, "//*[contains(text(), 'Xem th√™m')]")
            driver.execute_script("arguments[0].click();", see_more_btn)
            time.sleep(1) # ƒê·ª£i 1 gi√¢y ƒë·ªÉ n·ªôi dung hi·ªÉn th·ªã.
        except:
            pass # N·∫øu kh√¥ng c√≥ n√∫t n√†y th√¨ b·ªè qua

        # Kh·ªüi t·∫°o dictionary theo ƒë√∫ng format c·ªßa nh√≥m
        job_data = {
            "source": "vietnamworks",
            "url": url,
            "crawled_date": datetime.now(),
            "skills_tags": []
        }

        try:
            # 1. job_title
            try:
                job_data['job_title'] = driver.find_element(By.TAG_NAME, "h1").text.strip()
            except:
                job_data['job_title'] = "Unknown Title"

            # 2. company_name
            try:
                comp_elem = driver.find_element(By.XPATH, "//a[contains(@href, '/nha-tuyen-dung/')]")
                job_data['company_name'] = comp_elem.text.strip()
            except:
                job_data['company_name'] = "Unknown Company"

            # 3. salary_raw
            try:
                salary_elem = driver.find_element(By.XPATH, "//span[contains(@class, 'cVbwLK')]")
                job_data['salary_raw'] = salary_elem.text.strip()
            except:
                try:
                    job_data['salary_raw'] = driver.find_element(By.XPATH, "//*[contains(text(), 'Th∆∞∆°ng l∆∞·ª£ng')]").text
                except:
                    job_data['salary_raw'] = None

            # 4. location_raw
            try:
                locations = [
                    "An Giang", "B·∫Øc Ninh", "C√† Mau", "Cao B·∫±ng", "ƒêi·ªán Bi√™n", "ƒê·∫Øk L·∫Øk", 
                    "ƒê·ªìng Nai", "ƒê·ªìng Th√°p", "Gia Lai", "H√† Tƒ©nh", "H∆∞ng Y√™n", "Kh√°nh H√≤a", 
                    "Lai Ch√¢u", "L·∫°ng S∆°n", "L√†o Cai", "L√¢m ƒê·ªìng", "Ngh·ªá An", "Ninh B√¨nh", 
                    "Ph√∫ Th·ªç", "Qu·∫£ng Ng√£i", "Qu·∫£ng Ninh", "Qu·∫£ng Tr·ªã", "S∆°n La", "T√¢y Ninh", 
                    "Th√°i Nguy√™n", "Thanh H√≥a", "C·∫ßn Th∆°", "ƒê√† N·∫µng", "H√† N·ªôi", 
                    "H·∫£i Ph√≤ng", "H·ªì Ch√≠ Minh", "Hu·∫ø", "Tuy√™n Quang", "Vƒ©nh Long"
                ]
                #T·∫°o chu·ªói ƒëi·ªÅu ki·ªán OR cho c√°c th√†nh ph·ªë
                city_conditions = " or ".join([f"contains(text(), '{city}')" for city in locations])
                full_xpath = f"//*[contains(@class, 'ePOHWr') and ({city_conditions})]"
                location_elem = driver.find_element(By.XPATH, full_xpath)
                job_data['location_raw'] = location_elem.text.strip()
            except:
                job_data['location_raw'] = None

            # 5. posted_date
            job_data['posted_date'] = self.get_text_by_label(driver, "NG√ÄY ƒêƒÇNG")
            
            # 6. job_level
            job_data['job_level'] = self.get_text_by_label(driver, "C·∫§P B·∫¨C")
            
            # 7. experience_raw 
            # T√¨m theo "KINH NGHI·ªÜM" ho·∫∑c "S·ªê NƒÇM KINH NGHI·ªÜM"
            exp = self.get_text_by_label(driver, "KINH NGHI·ªÜM")
            if not exp:
                exp = self.get_text_by_label(driver, "S·ªê NƒÇM") # Fallback cho "S·ªê NƒÇM KINH NGHI·ªÜM T·ªêI THI·ªÇU"
            job_data['experience_raw'] = exp

            # 8. education_raw (Tr√¨nh ƒë·ªô h·ªçc v·∫•n)
            # Map t·ª´ "TR√åNH ƒê·ªò H·ªåC V·∫§N" ho·∫∑c "TR√åNH ƒê·ªò H·ªåC V·∫§N T·ªêI THI·ªÇU"
            job_data['education_raw'] = self.get_text_by_label(driver, "H·ªåC V·∫§N") 

            # 9. contract_type (Lo·∫°i h·ª£p ƒë·ªìng) 
            # L·∫•y t·ª´ "LO·∫†I H√åNH L√ÄM VI·ªÜC" (To√†n th·ªùi gian/B√°n th·ªùi gian)
            job_data['contract_type'] = self.get_text_by_label(driver, "LO·∫†I H√åNH L√ÄM VI·ªÜC")

            # 10. work_type (L√†m t·∫°i c√¥ng ty / Remote)
            #Vietnamworks √≠t ghi r√µ, t·∫°m ƒë·ªÉ NULL
            job_data['work_type'] = None

            # 11. english_req 
            #C≈©ng √≠t khi ghi r√µ, t·∫°m NULL
            job_data['english_req'] = None

            # 12. skills_tags
            skills_str = self.get_text_by_label(driver, "K·ª∏ NƒÇNG")
            if skills_str:
                job_data['skills_tags'] = [s.strip() for s in skills_str.split(',') if s.strip()]

            # 13. requirements_text (M√¥ t·∫£ c√¥ng vi·ªác)
            try:
                req_elem = driver.find_element(By.XPATH, "//h2[contains(text(), 'M√¥ t·∫£ c√¥ng vi·ªác')]/following-sibling::div")
                job_data['requirements_text'] = req_elem.text.strip()
            except:
                job_data['requirements_text'] = ""

            self.logger.info(f"‚úÖ ƒê√£ c√†o: {job_data['job_title']}")
            return job_data

        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi parse trang chi ti·∫øt {url}: {e}")
            return None

    def scrape(self):
        self.logger.info("üöÄ B·∫Øt ƒë·∫ßu c√†o VietnamWorks")
        driver = self.setup_driver()
        
        base_url = "https://www.vietnamworks.com/viec-lam?g=5&j=35.28.27.31.29.36.34.30.26.32.38"
        total_pages = 5 
        
        try:
            for page in range(1, total_pages + 1):
                current_url = f"{base_url}&page={page}"
                self.logger.info(f"üìÑ ƒêang qu√©t trang danh s√°ch s·ªë {page}...")
                
                job_links = self.get_job_links_on_page(driver, current_url)
                self.logger.info(f"   -> T√¨m th·∫•y {len(job_links)} vi·ªác l√†m.")
                
                for link in job_links:
                    # Ki·ªÉm tra xem URL n√†y ƒë√£ c√≥ trong database ch∆∞a
                    if self.collection.count_documents({'url': link}, limit=1) > 0:
                        self.logger.info(f"‚è© ƒê√£ t·ªìn t·∫°i, b·ªè qua: {link}")
                        continue # Nh·∫£y sang job ti·∫øp theo ngay, kh√¥ng v√†o parse n·ªØa

                    try:
                        job_detail = self.parse_job_detail(driver, link)
                        if job_detail:
                            self.save_job(job_detail)
                    except Exception as e:
                        self.logger.error(f"Skipping job: {e}")
        
        except Exception as global_e:
            self.logger.error(f"Global Error: {global_e}")

        finally:
            driver.quit()
            self.logger.info("üéâ Ho√†n th√†nh.")

if __name__ == "__main__":
    bot = VietnamWorksScraper()
    bot.scrape()