import time
import sys
import random
import math
from datetime import datetime
from playwright.sync_api import sync_playwright
from base_scraper import BaseScraper 

class GlintsScraper(BaseScraper):
    def __init__(self):
        # 1. Káº¿ thá»«a chuáº©n tá»« Leader
        super().__init__(source_name="glints", db_name="ADYM", collection_name="glints_tnth")
        
        # 2. Äáº£m báº£o Index duy nháº¥t Ä‘á»ƒ cháº·n trÃ¹ng URL tuyá»‡t Ä‘á»‘i
        self.collection.create_index("url", unique=True)
        
        self.ua_list = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        ]

    def scrape(self):
        self._crawl_missing_details()
        """Logic chÃ­nh tÃ­ch há»£p tá»« main cÅ©: Cháº¡y vÃ´ háº¡n vÃ  chia 2 Phase"""
        keywords = ["Frontend", "Backend", "Java", "Python", "NodeJS", 
                    "Full Stack", "Mobile Developer", "Data Scientist", 
                    "Business Analyst", "Tester", "Project Manager", "AI", "Machine Learning", "IT"]
        while True:
            self.logger.info("ğŸŒŠ [PHASE 1] Báº®T Äáº¦U QUÃ‰T METADATA CHO Táº¤T Cáº¢ Tá»ª KHÃ“A...")
            
            for kw in keywords:
                self.logger.info(f"ğŸ¯ Target: {kw.upper()}")
                
                for page_num in range(1, 11): 
                    # Láº¥y danh sÃ¡ch vÃ  tá»•ng sá»‘ trang thá»±c táº¿
                    jobs, total_pages = self._scrape_list_page(page_num, kw)
                    
                    # Náº¿u bá»‹ cháº·n (jobs is None), chuyá»ƒn tá»« khÃ³a ngay
                    if jobs is None:
                        self.logger.warning(f"âš ï¸ {kw} bá»‹ káº¹t á»Ÿ trang {page_num}. Chuyá»ƒn má»¥c tiÃªu.")
                        break
                    
                    # Náº¿u háº¿t dá»¯ liá»‡u thá»±c táº¿
                    if len(jobs) == 0:
                        self.logger.info(f"â¹ï¸ ÄÃ£ háº¿t tin cho '{kw}'.")
                        break
                    
                    self.logger.info(f"ğŸ“Š Trang {page_num} tráº£ vá» {len(jobs)} tin")

                    for job in jobs:
                        # LÆ°u báº±ng hÃ m cá»§a Leader (tá»± thÃªm source, crawled_date)
                        self.save_job(job)
                    
                    # Sá»¬A Lá»–I: Chá»‰ dá»«ng khi page_num thá»±c sá»± lá»›n hÆ¡n hoáº·c báº±ng total_pages (vá»›i total_pages > 0)
                    if total_pages > 0 and page_num >= total_pages:
                        self.logger.info(f"âœ… ÄÃ£ quÃ©t xong táº¥t cáº£ cÃ¡c trang cá»§a {kw}")
                        break
                    
                    time.sleep(random.randint(5, 10))

            # --- PHASE 2: VÃ Dá»® LIá»†U CHI TIáº¾T ---
            self.logger.info("ğŸ§¹ [PHASE 2] Báº®T Äáº¦U VÃ DETAIL...")
            self._crawl_missing_details()

            # --- PHASE 3: NGHá»ˆ NGÆ I Tá»”NG THá»‚ (15 phÃºt) ---
            self.logger.info(f"ğŸ˜´ Chu ká»³ hoÃ n táº¥t lÃºc {time.ctime()}. Nghá»‰ 15 phÃºt...")
            time.sleep(900)

    def _scrape_list_page(self, page_num, keyword):
        """BÃ³c tÃ¡ch danh sÃ¡ch vÃ  tÃ­nh toÃ¡n láº¡i total_pages chuáº©n xÃ¡c"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(user_agent=random.choice(self.ua_list))
            page_obj = context.new_page()
            url = f"https://glints.com/vn/opportunities/jobs/explore?keyword={keyword}&country=VN&page={page_num}"
            
            try:
                def is_search_api(res):
                    return "graphql" in res.url and "searchJobsV3" in res.url and res.status == 200

                with page_obj.expect_response(is_search_api, timeout=30000) as response_info:
                    page_obj.goto(url, wait_until="networkidle", timeout=60000)
                    page_obj.mouse.wheel(0, 500) 
                    time.sleep(2)

                data = response_info.value.json()
                search_data = data.get('data', {}).get('searchJobsV3', {})
                
                # Sá»¬A Lá»–I TÃNH TRANG: DÃ¹ng math.ceil Ä‘á»ƒ lÃ m trÃ²n lÃªn chuáº©n xÃ¡c
                total_jobs = search_data.get('totalJobCount', 0)
                total_pages = math.ceil(total_jobs / 30) if total_jobs > 0 else 0
                
                raw_list = search_data.get('jobsInPage', [])
                processed = []
                for r in raw_list:
                    loc = r.get('location', {})
                    parents = loc.get('parents', [])
                    full_loc = f"{loc.get('name', '')}, {parents[0].get('name', '') if parents else ''}".strip(", ")
                    
                    # Cáº¥u trÃºc Dictionary khá»›p y há»‡t hÃ¬nh áº£nh Database cá»§a báº¡n
                    processed.append({
                        "url": f"https://glints.com/vn/opportunities/jobs/{r.get('id')}",
                        "job_title": r.get('title'),
                        "company_name": (r.get('company') or {}).get('name'),
                        "salary_raw": "Thá»a thuáº­n",
                        "location_raw": full_loc or "Viá»‡t Nam",
                        "work_type": None,
                        "job_level": None,
                        "experience_raw": f"{r.get('minYearsOfExperience', 0)}-{r.get('maxYearsOfExperience', 0)} nÄƒm",
                        "education_raw": r.get('educationLevel'),
                        "english_req": None,
                        "requirements_text": None,
                        "skills_tags": [s['skill']['name'] for s in r.get('skills', []) if 'skill' in s],
                        "source": self.source,
                        "posted_date": r.get('createdAt')
                    })
                return processed, total_pages
            except Exception as e:
                self.logger.error(f"âŒ Lá»—i trang {page_num}: {e}")
                return None, 0
            finally:
                browser.close()

    def _crawl_missing_details(self):
        """VÃ¡ dá»¯ liá»‡u chi tiáº¿t cho cÃ¡c tin chÆ°a cÃ³ requirements_text"""
        query = {"source": self.source, "requirements_text": None}
        missing_jobs = list(self.collection.find(query).limit(50)) 

        if not missing_jobs:
            self.logger.info("âœ… Database Ä‘Ã£ Ä‘áº§y Ä‘á»§ thÃ´ng tin!")
            return

        for job in missing_jobs:
            try:
                job_id = job['url'].split('/')[-1].split('?')[0]
                detail_data = self._scrape_detail_logic(job_id)
                
                if detail_data:
                    self.collection.update_one({"_id": job["_id"]}, {"$set": detail_data})
                    self.logger.info(f"   âœ… ÄÃ£ vÃ¡: {job.get('job_title')}")
                else:
                    self.logger.warning("ğŸ›‘ Glints cháº·n vÃ¡ tin. Nghá»‰ 15 phÃºt...")
                    time.sleep(900)
                    break 
                
                time.sleep(random.uniform(8, 15)) 
            except Exception as e:
                self.logger.error(f"âŒ Lá»—i vÃ¡ ID {job['_id']}: {e}")

    def _scrape_detail_logic(self, job_id):
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
            page = context.new_page()
            page.add_init_script(
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            )

            url = f"https://glints.com/vn/opportunities/jobs/{job_id}"
            try:
                response = page.goto(url, wait_until="networkidle", timeout=60000)
                if response.status in [403, 400]:
                    return None

                # 1. GTM + DOM metadata
                company_size = "N/A"
                salary_from_dom = "Thá»a thuáº­n"

                gtm_btn = page.locator('.job_details-apply_button').first
                if gtm_btn.count() > 0:
                    company_size = gtm_btn.get_attribute(
                        'data-gtm-company-size'
                    ) or "N/A"

                salary_loc = page.locator(
                    'span[class*="SalaryWrapper"], div[class*="SalaryJobOverview"], .lcEIyF'
                ).first
                if salary_loc.count() > 0:
                    text = salary_loc.inner_text().strip()
                    if "VND" in text or "Tr" in text:
                        salary_from_dom = text.replace("/ThÃ¡ng", "").strip()

                # 2. DOM DETAIL (nguá»“n chÃ­nh)
                title = (
                    page.locator('h1[class*="JobOverViewTitle"]').inner_text().strip()
                    if page.locator('h1[class*="JobOverViewTitle"]').count() > 0
                    else "N/A"
                )

                info_nodes = page.locator(
                    'div[class*="JobOverViewInfo"]'
                ).all_inner_texts()
                c_type, w_type = "N/A", "N/A"
                for text in info_nodes:
                    if " Â· " in text:
                        parts = text.split(" Â· ")
                        c_type = parts[0].strip()
                        w_type = parts[1].strip()

                requirements_text = (
                    page.locator('div[class*="JobDescriptionContainer"]')
                    .inner_text()
                    .strip()
                    if page.locator('div[class*="JobDescriptionContainer"]').count() > 0
                    else "N/A"
                )

                return {
                    "job_title": title,
                    "company_size": company_size,
                    "contract_type": c_type,
                    "work_type": w_type,
                    "requirements_text": requirements_text,
                    "salary_raw": salary_from_dom
                }

            except Exception as e:
                print(f"âš ï¸ Lá»—i: {e}")
                return None
            finally:
                browser.close()

if __name__ == "__main__":
    scraper = GlintsScraper()
    try:
        scraper.scrape()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ÄÃ£ nháº­n lá»‡nh dá»«ng. Äang Ä‘Ã³ng káº¿t ná»‘i...")
        scraper.close_connection() # DÃ¹ng hÃ m Ä‘Ã³ng káº¿t ná»‘i cá»§a Leader
        sys.exit(0)