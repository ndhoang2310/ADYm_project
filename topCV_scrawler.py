import time
import random 
from datetime import datetime
from seleniumbase import Driver
from base_scraper import BaseScraper 

class TopCVScraper(BaseScraper):
    def __init__(self, start_url):
        # Kh·ªüi t·∫°o v·ªõi collection ri√™ng c·ªßa b·∫°n
        super().__init__(source_name="topcv", collection_name="topcv_Hoang_17_2")
        self.base_url = start_url
        print("üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát (UC Mode - Enhanced)...")
        self.driver = Driver(uc=True, headless=False)

    def is_url_exists(self, url):
        """Ki·ªÉm tra URL ƒë√£ t·ªìn t·∫°i trong MongoDB ch∆∞a (X√¢y d·ª±ng t·∫°i class n√†y)"""
        return self.collection.find_one({"url": url}) is not None

    def get_text_js(self, selector):
        script = f'var el = document.querySelector("{selector}"); return el ? el.innerText.trim() : null;'
        return self.driver.execute_script(script)

    def get_job_title_safely(self):
        selectors = ["h1[class*='job-detail__info--title']", ".job-detail__info--title", "h1.title"]
        for s in selectors:
            if self.driver.is_element_visible(s):
                text = self.get_text_js(s)
                if text: return text
        return None

    def get_company_name_safely(self):
        selectors = [".company-name-label a.name", ".company-name-label a", "a[href*='/cong-ty/'] .name"]
        for s in selectors:
            if self.driver.is_element_visible(s):
                text = self.get_text_js(s)
                if text: return text
        return None

    def extract_header_summary(self):
        """L·∫•y L∆∞∆°ng, ƒê·ªãa ƒëi·ªÉm, Kinh nghi·ªám t·ª´ c√°c box icon"""
        results = {"salary_raw": None, "location_raw": "N/A", "experience_raw": None}
        mapping = {"m·ª©c l∆∞∆°ng": "salary_raw", "ƒë·ªãa ƒëi·ªÉm": "location_raw", "kinh nghi·ªám": "experience_raw"}
        
        sections = self.driver.find_elements(".job-detail__info--section")
        for section in sections:
            try:
                title = section.find_element(by="css selector", value=".job-detail__info--section-content-title").text.lower()
                value = section.find_element(by="css selector", value=".job-detail__info--section-content-value").text.strip()
                for label, key in mapping.items():
                    if label in title: results[key] = value
            except: continue
        return results

    def extract_general_info_dynamic(self):
        """L·∫•y C·∫•p b·∫≠c, H·ªçc v·∫•n, H√¨nh th·ª©c l√†m vi·ªác"""
        data = {"job_level": None, "education_raw": None, "work_type": None}
        label_map = {"c·∫•p b·∫≠c": "job_level", "h·ªçc v·∫•n": "education_raw", "h√¨nh th·ª©c": "work_type"}

        groups = self.driver.find_elements(".box-general-group-info")
        for group in groups:
            try:
                title = group.find_element(by="css selector", value=".box-general-group-info-title").text.lower()
                value = group.find_element(by="css selector", value=".box-general-group-info-value").text.strip()
                for label, key in label_map.items():
                    if label in title: data[key] = value
            except: continue
        return data

    def extract_content_blocks(self):
        """L·∫•y M√¥ t·∫£, Y√™u c·∫ßu v√† Quy·ªÅn l·ª£i (Kh√¥i ph·ª•c ƒë·∫ßy ƒë·ªß)"""
        data = {"job_description": None, "requirements_text": "N/A", "benefits": None}
        blocks = self.driver.find_elements(".job-description__item")
        for block in blocks:
            try:
                title = block.find_element(by="css selector", value="h3").text.lower()
                content = block.find_element(by="css selector", value=".job-description__item--content").text.strip()
                if "m√¥ t·∫£" in title: data["job_description"] = content
                elif "y√™u c·∫ßu" in title: data["requirements_text"] = content
                elif "quy·ªÅn l·ª£i" in title: data["benefits"] = content
            except: continue
        return data

    def scrape_job_detail(self, url):
        """C√†o chi ti·∫øt m·ªôt c√¥ng vi·ªác"""
        try:
            self.driver.get(url)
            self.driver.wait_for_element("h1[class*='job-detail__info--title']", timeout=12)
            time.sleep(1.5)

            job_title = self.get_job_title_safely()
            company_name = self.get_company_name_safely()
            if not job_title: return None

            item = {
                "url": url,
                "source": "topcv",
                "job_title": job_title,
                "company_name": company_name,
                "crawled_date": datetime.now(),
                "posted_date": self.get_text_js(".job-detail__info--deadline-date")
            }

            # Gom d·ªØ li·ªáu t·ª´ c√°c h√†m b√≥c t√°ch
            header_info = self.extract_header_summary()
            general_info = self.extract_general_info_dynamic()
            content_blocks = self.extract_content_blocks()
            
            item.update(header_info)
            item.update(general_info)
            item.update(content_blocks)

            # --- X·ª¨ L√ù SKILL TAGS K·ª∏ L∆Ø·ª†NG ---
            tags = self.driver.find_elements(".box-category-tag")
            raw_tags = [t.text.strip() for t in tags if t.text.strip()]
            
            # 1. Danh s√°ch lo·∫°i tr·ª´ d·ª±a tr√™n d·ªØ li·ªáu metadata ƒë√£ l·∫•y
            meta_values = [
                str(item.get('location_raw', '')).lower(),
                str(item.get('work_type', '')).lower(),
                str(item.get('job_level', '')).lower(),
                str(item.get('experience_raw', '')).lower(),
                str(item.get('education_raw', '')).lower()
            ]

            # 2. Blacklist c√°c t·ª´ kh√≥a r√°c th∆∞·ªùng xu·∫•t hi·ªán trong tag c·ªßa TopCV
            junk_blacklist = [
                'h√† n·ªôi', 'h·ªì ch√≠ minh', 'ƒë√† n·∫µng', 'hcm', 'to√†n qu·ªëc', 'mi·ªÅn nam', 'mi·ªÅn b·∫Øc',
                'nh√¢n vi√™n', 'tr∆∞·ªüng nh√≥m', 'tr∆∞·ªüng ph√≤ng', 'gi√°m ƒë·ªëc', 'th·ª±c t·∫≠p', 'fresher', 'junior', 'senior',
                'to√†n th·ªùi gian', 'b√°n th·ªùi gian', 'th√°ng', 'nƒÉm', 'ng∆∞·ªùi', 'tri·ªáu', 'vnd', 'usd',
                'th·ªèa thu·∫≠n', 'c·∫°nh tranh', 'ƒë·∫°i h·ªçc', 'cao ƒë·∫≥ng', 'h·∫°n n·ªôp', 'quy m√¥', 'v·ªã tr√≠'
            ]

            filtered_skills = []
            for tag in list(set(raw_tags)):
                tag_lower = tag.lower()
                # Ki·ªÉm tra xem tag c√≥ n·∫±m trong metadata ho·∫∑c blacklist kh√¥ng
                is_meta = any(val in tag_lower or tag_lower in val for val in meta_values if val)
                is_junk = any(junk in tag_lower for junk in junk_blacklist)
                
                if not is_meta and not is_junk:
                    filtered_skills.append(tag)

            item['skills_tags'] = filtered_skills
            # ---------------------------------
            
            if item.get('work_type'):
                wt = item['work_type'].lower()
                item['contract_type'] = 'Full-time' if 'to√†n th·ªùi gian' in wt else ('Part-time' if 'b√°n th·ªùi gian' in wt else item['work_type'])
            
            return item
        except Exception as e:
            print(f"‚ùå L·ªói khi c√†o chi ti·∫øt {url}: {e}")
            return None

    def scrape(self):
        page_num = 1
        while True:
            current_url = f"{self.base_url}&page={page_num}"
            print(f"\n--- üìÑ ƒêang qu√©t trang {page_num}: {current_url} ---")
            
            self.driver.get(current_url)
            time.sleep(random.uniform(5, 8))

            if self.driver.is_element_visible(".empty-job-list") or self.driver.is_text_visible("Ch∆∞a t√¨m th·∫•y vi·ªác l√†m"):
                break

            links = [el.get_attribute("href") for el in self.driver.find_elements(".job-item-search-result .title a") if el.get_attribute("href")]
            links = list(set(links))
            print(f"‚úÖ T√¨m th·∫•y {len(links)} link. ƒêang l·ªçc tr√πng...")

            for link in links:
                # Ki·ªÉm tra tr√πng t·∫°i ƒë√¢y tr∆∞·ªõc khi c√†o chi ti·∫øt
                if self.is_url_exists(link):
                    print(f"   ‚è© B·ªè qua (ƒê√£ t·ªìn t·∫°i): {link[:50]}...")
                    continue

                data = self.scrape_job_detail(link)
                if data:
                    self.save_job(data)
                    print(f"   ‚úîÔ∏è ƒê√£ l∆∞u: {data['job_title'][:40]}...")
                time.sleep(random.uniform(1, 3))

            page_num += 1
        
        self.driver.quit()
        self.close_connection()

if __name__ == "__main__":
    url_it = "https://www.topcv.vn/tim-viec-lam-cong-nghe-thong-tin-cr257?type_keyword=1"
    bot = TopCVScraper(start_url=url_it)
    bot.scrape()